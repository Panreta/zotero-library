@misc{chenReshapingReasoningLLMs2025,
  title = {Reshaping {{Reasoning}} in {{LLMs}}: {{A Theoretical Analysis}} of {{RL Training Dynamics}} through {{Pattern Selection}}},
  shorttitle = {Reshaping {{Reasoning}} in {{LLMs}}},
  author = {Chen, Xingwu and Li, Tianle and Zou, Difan},
  year = 2025,
  month = sep,
  number = {arXiv:2506.04695},
  eprint = {2506.04695},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.04695},
  urldate = {2025-11-01},
  abstract = {Reinforcement learning (RL) has demonstrated remarkable success in enhancing model capabilities, including instruction-following, preference learning, and reasoning. Yet despite its empirical successes, the mechanisms by which RL improves reasoning abilities remain poorly understood. We present a systematic study of Reinforcement Learning with Verifiable Rewards (RLVR), showing that its primary benefit comes from optimizing the selection of existing reasoning patterns. Through extensive experiments, we demonstrate that RLVR-trained models preferentially adopt high-success-rate reasoning patterns while mostly maintaining stable performance on individual patterns. We further develop theoretical analyses on the convergence and training dynamics of RLVR based on a simplified question-reasonanswer model. We study the gradient flow and show that RLVR can indeed find the solution that selects the reason pattern with the highest success rate. Besides, our theoretical results reveal two distinct regimes regarding the convergence of RLVR training: (1) rapid convergence for models with relatively strong initial reasoning capabilities versus (2) slower optimization dynamics for weaker models. Furthermore, we show that the slower optimization for weaker models can be mitigated by applying the supervised fine-tuning (SFT) before RLVR, when using a feasibly high-quality SFT dataset. We validate the theoretical findings through extensive experiments. This work advances our theoretical understanding of RL's role in LLM fine-tuning and offers insights for further enhancing reasoning capabilities.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\tl142\Zotero\storage\384HWI5I\Chen et al. - 2025 - Reshaping Reasoning in LLMs A Theoretical Analysis of RL Training Dynamics through Pattern Selectio.pdf}
}

@article{deanMapReduceSimplifiedData2008,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = 2008,
  month = jan,
  journal = {Communications of the ACM},
  volume = {51},
  number = {1},
  pages = {107--113},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1327452.1327492},
  urldate = {2026-01-10},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper.},
  langid = {english},
  file = {C:\Users\tl142\Zotero\storage\27XFCMLP\Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clusters.pdf}
}

@misc{estebanRealvaluedMedicalTime2017,
  title = {Real-Valued ({{Medical}}) {{Time Series Generation}} with {{Recurrent Conditional GANs}}},
  author = {Esteban, Crist{\'o}bal and Hyland, Stephanie L. and R{\"a}tsch, Gunnar},
  year = 2017,
  month = dec,
  number = {arXiv:1706.02633},
  eprint = {1706.02633},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.02633},
  urldate = {2026-01-11},
  abstract = {Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data. In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data. RGANs make use of recurrent neural networks (RNNs) in the generator and the discriminator. In the case of RCGANs, both of these RNNs are conditioned on auxiliary information. We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series. We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa. We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data. This is demonstrated on digit classification from `serialised' MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit. We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data, and demonstrate results from differentially private training of the RCGAN.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\tl142\Zotero\storage\YSNWK7SK\Esteban et al. - 2017 - Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs.pdf}
}

@article{ongaroSearchUnderstandableConsensus,
  title = {In {{Search}} of an {{Understandable Consensus Algorithm}}},
  author = {Ongaro, Diego and Ousterhout, John},
  abstract = {Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandability, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.},
  langid = {english},
  file = {C:\Users\tl142\Zotero\storage\S5NGCBWF\Ongaro and Ousterhout - In Search of an Understandable Consensus Algorithm.pdf}
}

@misc{teamKimiLinearExpressive2025,
  title = {Kimi {{Linear}}: {{An Expressive}}, {{Efficient Attention Architecture}}},
  shorttitle = {Kimi {{Linear}}},
  author = {Team, Kimi and Zhang, Yu and Lin, Zongyu and Yao, Xingcheng and Hu, Jiaxi and Meng, Fanqing and Liu, Chengyin and Men, Xin and Yang, Songlin and Li, Zhiyuan and Li, Wentao and Lu, Enzhe and Liu, Weizhou and Chen, Yanru and Xu, Weixin and Yu, Longhui and Wang, Yejie and Fan, Yu and Zhong, Longguang and Yuan, Enming and Zhang, Dehao and Zhang, Yizhi and Liu, T. Y. and Wang, Haiming and Fang, Shengjun and He, Weiran and Liu, Shaowei and Li, Yiwei and Su, Jianlin and Qiu, Jiezhong and Pang, Bo and Yan, Junjie and Jiang, Zhejun and Huang, Weixiao and Yin, Bohong and You, Jiacheng and Wei, Chu and Wang, Zhengtao and Hong, Chao and Chen, Yutian and Chen, Guanduo and Wang, Yucheng and Zheng, Huabin and Wang, Feng and Liu, Yibo and Dong, Mengnan and Zhang, Zheng and Pan, Siyuan and Wu, Wenhao and Wu, Yuhao and Guan, Longyu and Tao, Jiawen and Fu, Guohong and Xu, Xinran and Wang, Yuzhi and Lai, Guokun and Wu, Yuxin and Zhou, Xinyu and Yang, Zhilin and Du, Yulun},
  year = 2025,
  month = nov,
  number = {arXiv:2510.26692},
  eprint = {2510.26692},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.26692},
  urldate = {2026-01-01},
  abstract = {We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios---including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet [111] with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-LowRank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule. We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75\% and achieving up to 6\texttimes{} decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\tl142\Zotero\storage\LG2FRNK6\Team et al. - 2025 - Kimi Linear An Expressive, Efficient Attention Architecture.pdf}
}

@article{wangConciseProofBenfords2024,
  title = {A Concise Proof of {{Benford}}'s Law},
  author = {Wang, Luohan and Ma, Bo-Qiang},
  year = 2024,
  month = jul,
  journal = {Fundamental Research},
  volume = {4},
  number = {4},
  pages = {841--844},
  issn = {26673258},
  doi = {10.1016/j.fmre.2023.01.002},
  urldate = {2026-01-10},
  abstract = {This article presents a concise proof of the famous Benford's law when the distribution has a Riemann integrable probability density function and provides a criterion to judge whether a distribution obeys the law. The proof is intuitive and elegant, accessible to anyone with basic knowledge of calculus, revealing that the law originates from the basic property of human number system. The criterion can bring great convenience to the field of fraud detection.},
  langid = {english},
  file = {C:\Users\tl142\Zotero\storage\DJ78FL7R\Wang and Ma - 2024 - A concise proof of Benford’s law.pdf}
}

@misc{wangExecutableCodeActions2024,
  title = {Executable {{Code Actions Elicit Better LLM Agents}}},
  author = {Wang, Xingyao and Chen, Yangyi and Yuan, Lifan and Zhang, Yizhe and Li, Yunzhu and Peng, Hao and Ji, Heng},
  year = 2024,
  month = jun,
  number = {arXiv:2402.01030},
  eprint = {2402.01030},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.01030},
  urldate = {2026-01-11},
  abstract = {Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on APIBank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20\% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\tl142\Zotero\storage\KGPYA38R\Wang et al. - 2024 - Executable Code Actions Elicit Better LLM Agents.pdf}
}

@misc{xieMHCManifoldConstrainedHyperConnections2025,
  title = {{{mHC}}: {{Manifold-Constrained Hyper-Connections}}},
  shorttitle = {{{mHC}}},
  author = {Xie, Zhenda and Wei, Yixuan and Cao, Huanqi and Zhao, Chenggang and Deng, Chengqi and Li, Jiashi and Dai, Damai and Gao, Huazuo and Chang, Jiang and Zhao, Liang and Zhou, Shangyan and Xu, Zhean and Zhang, Zhengyan and Zeng, Wangding and Hu, Shengding and Wang, Yuqing and Yuan, Jingyang and Wang, Lean and Liang, Wenfeng},
  year = 2025,
  month = dec,
  number = {arXiv:2512.24880},
  eprint = {2512.24880},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2512.24880},
  urldate = {2026-01-01},
  abstract = {Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\tl142\Zotero\storage\TU382GUD\Xie et al. - 2025 - mHC Manifold-Constrained Hyper-Connections.pdf}
}

@article{zhouConcurrencyControlService2025,
  title = {Concurrency {{Control}} as a {{Service}}},
  author = {Zhou, Weixing and Zhang, Yanfeng and Zhou, Xinji and Wang, Zhiyou and Peng, Zeshun and Ren, Yang and Li, Sihao and Zhang, Huanchen and Li, Guoliang and Yu, Ge},
  year = 2025,
  month = may,
  journal = {Proceedings of the VLDB Endowment},
  volume = {18},
  number = {9},
  pages = {2761--2774},
  issn = {2150-8097},
  doi = {10.14778/3746405.3746406},
  urldate = {2025-11-01},
  abstract = {Existing disaggregated databases separate execution and storage layers, enabling independent and elastic scaling of resources. In most cases, this design makes transaction concurrency control (CC) a critical bottleneck, which demands significant computing resources for concurrent conflict management and struggles to scale due to the coordination overhead for concurrent conflict resolution. Coupling CC with execution or storage limits performance and elasticity, as CC's resource needs do not align with the free scaling of the transaction execution layer or the storage-bound data layer. This paper proposes Concurrency Control as a Service (CCaaS), which decouples CC from databases, building an execution-CCstorage three-layer decoupled database, allowing independent scaling and upgrades for improved elasticity, resource utilization, and development agility. However, adding a new layer increases latency due to the shift in communication from hardware to network. To address this, we propose a Sharded Multi-Write OCC (SM-OCC) algorithm with an asynchronous log push-down mechanism to minimize network communications overhead and transaction latency. Additionally, we implement a multi-write architecture with a deterministic conflict resolution method to reduce coordination overhead in the CC layer, thereby improving scalability. CCaaS is designed to be connected by a variety of execution and storage engines. Existing disaggregated databases can be revolutionized with CCaaS to achieve high elasticity, scalability, and high performance. Results show that CCaaS achieves 1.02-3.11\texttimes{} higher throughput and 1.11-2.75\texttimes{} lower latency than SoTA disaggregated databases.},
  langid = {english},
  file = {C:\Users\tl142\Zotero\storage\BFJW2ZPL\Zhou 等 - 2025 - Concurrency Control as a Service.pdf}
}
